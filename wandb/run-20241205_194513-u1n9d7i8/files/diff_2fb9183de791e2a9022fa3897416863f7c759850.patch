diff --git a/.DS_Store b/.DS_Store
index 6eed836..e23db1f 100644
Binary files a/.DS_Store and b/.DS_Store differ
diff --git a/__pycache__/environment.cpython-38.pyc b/__pycache__/environment.cpython-38.pyc
index f1cf6c7..aec8b9c 100644
Binary files a/__pycache__/environment.cpython-38.pyc and b/__pycache__/environment.cpython-38.pyc differ
diff --git a/__pycache__/utils.cpython-38.pyc b/__pycache__/utils.cpython-38.pyc
index 4a98824..a6b6a59 100644
Binary files a/__pycache__/utils.cpython-38.pyc and b/__pycache__/utils.cpython-38.pyc differ
diff --git a/__pycache__/visualize.cpython-38.pyc b/__pycache__/visualize.cpython-38.pyc
index caed1dc..e51cf68 100644
Binary files a/__pycache__/visualize.cpython-38.pyc and b/__pycache__/visualize.cpython-38.pyc differ
diff --git a/environment.py b/environment.py
index 72fe829..a09e105 100644
--- a/environment.py
+++ b/environment.py
@@ -13,14 +13,9 @@ from keyboard_control import *
 # Environment Parameters
 NUM_FOODS = 6  # Number of foods
 ENERGY_FACTOR = 20
-HOME_POSITION = (0, 0)  # Coordinates of the home
-HOME_SIZE = 2
-HOME_GRID_X = {HOME_POSITION[0] + i for i in range(HOME_SIZE)}
-HOME_GRID_Y = {HOME_POSITION[1] + i for i in range(HOME_SIZE)}
-
 NUM_ACTIONS = 6
 
-# print("HOME GRID X,Y", HOME_GRID_X, HOME_GRID_Y)
+# print("HOME GRID X,Y", self.home_grid_x, self.home_grid_y)
 MAX_MESSAGE_LENGTH = 10  # Example message length limit
 AGENT_ATTRIBUTES = [150]  # All agents have the same attributes
 HOME_ATTRIBUTES = [100]
@@ -36,7 +31,7 @@ collect_all_reward = 0
 pickup_reward = 0
 drop_punishment = 0
 drop_reward_factor = 0.1 # multiplying with energy
-energy_reward_factor = 0.01
+energy_reward_factor = 0.05
 
 class Environment(ParallelEnv):
     metadata = {"name": "multi_foraging"}
@@ -59,8 +54,15 @@ class Environment(ParallelEnv):
         self.action_spaces = spaces.Dict({i: self.single_action_space for i in range(num_agents)})
         self.render_mode = None
         
+        self.home_size = 2
+        self.reward_denom = 20 # normalize reward to be in range(-1,1)
 
     def reset(self, seed=42, options=None):
+        home_rand_pos = np.random.randint(self.grid_size-self.home_size, size=2)
+        self.home_position = (home_rand_pos[0], home_rand_pos[1])  # Coordinates of the home
+        self.home_grid_x = {self.home_position[0] + i for i in range(self.home_size)}
+        self.home_grid_y = {self.home_position[1] + i for i in range(self.home_size)}
+
         self.episode_lengths = {i:0 for i in range(len(self.possible_agents))}
         self.cumulative_rewards = {i:0 for i in range(len(self.possible_agents))}
         self.dones = {i:False for i in range(len(self.possible_agents))}
@@ -125,7 +127,7 @@ class Environment(ParallelEnv):
     def random_position(self):
         while True:
             pos = (random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1))
-            if self.grid[pos[0], pos[1]] is None and self.min_dist(pos,2) and self.compute_dist(pos, HOME_POSITION) > 2:
+            if self.grid[pos[0], pos[1]] is None and self.min_dist(pos,2) and self.compute_dist(pos, self.home_position) > 2:
                 self.prev_pos_list.append(pos)
                 return pos
 
@@ -168,7 +170,11 @@ class Environment(ParallelEnv):
 
         return action_map[action]
 
-
+    def normalize_reward(self, reward):
+        norm_reward = {}
+        for key, item in reward.items():
+            norm_reward[key] = item / self.reward_denom
+        return norm_reward
 
     def step(self, agent_actions, int_action=True):
         # Update food state: Clear all agents if not carried
@@ -267,8 +273,8 @@ class Environment(ParallelEnv):
                             loss = 0.2*min(self.agent_maps[agent_id].strength, self.agent_maps[agent_id].carrying_food.strength_required)
                             self.agent_maps[agent_id].energy -= loss # When carry food, agent lose more energy due to friction
                             # step_reward
-                            # old_home_dist = self.compute_dist(old_position, HOME_POSITION)
-                            # new_home_dist = self.compute_dist(new_position, HOME_POSITION)
+                            # old_home_dist = self.compute_dist(old_position, self.home_position)
+                            # new_home_dist = self.compute_dist(new_position, self.home_position)
                             # self.rewards[agent_id] += 0.2*(old_home_dist-new_home_dist)
 
                         if not(agent.carrying_food.is_moved):
@@ -312,8 +318,8 @@ class Environment(ParallelEnv):
 
             elif action == "drop" and agent.carrying_food:
                 # If agent drops food at home
-                if (agent.carrying_food.position[0] in range(HOME_POSITION[0], HOME_POSITION[0] + HOME_SIZE) and 
-                    agent.carrying_food.position[1] in range(HOME_POSITION[1], HOME_POSITION[1] + HOME_SIZE)):
+                if (agent.carrying_food.position[0] in range(self.home_position[0], self.home_position[0] + self.home_size) and 
+                    agent.carrying_food.position[1] in range(self.home_position[1], self.home_position[1] + self.home_size)):
                     
                     # Dismiss the dropped food item at home
                     agent.carrying_food.position = (-2000,-2000)
@@ -372,7 +378,8 @@ class Environment(ParallelEnv):
                                 },
                             }
                                          
-                                
+        # normalize reward
+        self.rewards = self.normalize_reward(self.rewards)
         return self.observe(), self.rewards, self.dones, self.truncated, self.infos
 
 # Define the classes
@@ -406,7 +413,7 @@ class EnvAgent:
                 if 0 <= x < self.grid_size and 0 <= y < self.grid_size:
                     obj = environment.grid[x, y]
                     if obj is None:
-                        if x in HOME_GRID_X and y in HOME_GRID_Y:
+                        if x in environment.home_grid_x and y in environment.home_grid_y:
                             row.append(HOME_ATTRIBUTES)  # home grid
                         else:
                             row.append([0])  # Empty grid
diff --git a/environment_old.py b/environment_old.py
deleted file mode 100644
index d9fc9bb..0000000
--- a/environment_old.py
+++ /dev/null
@@ -1,295 +0,0 @@
-import pygame
-import numpy as np
-import random
-import time
-
-from constants import *
-from keyboard_control import *
-
-# Environment Parameters
-GRID_SIZE = 10  # Size of the grid world
-NUM_AGENTS = 2  # Number of agents
-NUM_FOODS = 4  # Number of foods
-HOME_POSITION = (1, 1)  # Coordinates of the home
-MAX_MESSAGE_LENGTH = 10  # Example message length limit
-AGENT_ATTRIBUTES = [5, 5, 5, 5]  # All agents have the same attributes
-AGENT_STRENGTH = 3
-AGENT_ENERGY = 30
-
-MAX_REQUIRED_STRENGTH = 6
-HOME_SIZE = 2
-
-# Reward Hyperparameters
-energy_punishment = -30
-collect_all_reward = 200
-pickup_reward = 10
-drop_punishment = -10
-drop_reward = 1 # multiplying with energy
-# Define the classes
-class Agent:
-    def __init__(self, id, position, strength, max_energy):
-        self.id = id
-        self.position = position
-        self.strength = strength
-        self.energy = max_energy
-        self.carrying_food = None
-        self.done = False
-        self.messages = []
-        # Agent observation field adjusted to (24, 4) for the 5x5 grid view, exluding agent position
-    
-    def observe(self, environment):
-        # Define the 5x5 field of view around the agent, excluding its center
-        perception_data = []
-        for dx in range(-2, 3):
-            for dy in range(-2, 3):
-                if dx == 0 and dy == 0:
-                    perception_data.append([0, 0, 0, 0])  # agent's own position
-                    continue
-
-                x, y = self.position[0] + dx, self.position[1] + dy
-                if 0 <= x < GRID_SIZE and 0 <= y < GRID_SIZE:
-                    obj = environment.grid[x, y]
-                    if obj is None:
-                        perception_data.append([0, 0, 0, 0])  # Empty grid
-                    else:
-                        perception_data.append(obj.attribute)  # Food or other object attributes
-                else:
-                    perception_data.append([0, 0, 0, 0])  # Out-of-bounds grid (treated as empty)
-        
-        return np.array(perception_data)
-
-    def send_message(self, env):
-        return env.message[0,:] #TODO Use neural network to send message
-
-class Food:
-    def __init__(self, position, strength_required, id):
-        self.position = position
-        self.strength_required = strength_required
-        self.carried = [] # keep all agents that already picked up this food
-        self.pre_carried = [] # keep all agents that try to pick up this food, no need to be successful
-        self.attribute = self.generate_attributes(strength_required)
-        self.energy_score = 10 * strength_required
-        self.id = id
-        self.done = False
-        self.reduced_strength = 0
-
-    def generate_attributes(self, strength_required):
-        # Return unique attributes based on the food's strength requirement
-        attribute_mapping = {
-            1: [1, 1, 1, 1], # Spinach
-            2: [1, 1, 2, 2], # Watermelon
-            3: [1, 3, 3, 3], # Strawberry
-            4: [2, 4, 1, 4], # Chicken
-            5: [2, 6, 4, 3], # Pig
-            6: [2, 3, 8, 3], # Cattle
-
-        }
-        return np.array(attribute_mapping.get(strength_required, [1, 1, 1, 1]))
-
-class Environment:
-    def __init__(self):
-        self.reset()
-    def reset(self):
-        self.grid = np.full((GRID_SIZE, GRID_SIZE), None)
-        self.prev_pos_list = []
-        # Initialize agents with uniform attributes
-        self.agents = [Agent(i, self.random_position(), AGENT_STRENGTH, AGENT_ENERGY) for i in range(NUM_AGENTS)]
-        for agent in self.agents:
-            self.grid[agent.position[0], agent.position[1]] = agent
-        self.foods = [Food(self.random_position(), random.randint(1, MAX_REQUIRED_STRENGTH), food_id) for food_id in range(NUM_FOODS)]
-        for food in self.foods:
-            self.grid[food.position[0], food.position[1]] = food
-
-        self.collected_foods = set()
-        self.message = np.zeros((NUM_AGENTS, MAX_MESSAGE_LENGTH)) # Message that each agent sends, each agent receive N-1 agents' messages
-        return self.observe()
-
-    def update_grid(self):
-        self.grid = np.full((GRID_SIZE, GRID_SIZE), None)
-        for agent in self.agents:
-            if not(agent.done): # If agent is alive
-                self.grid[agent.position[0], agent.position[1]] = agent
-        for food in self.foods:
-            if not(food.done): # If food is not placed at home
-                self.grid[food.position[0], food.position[1]] = food
-
-    def min_dist(self,curr_pos, min_distance):
-        satisfy = True
-        for prev_pos in self.prev_pos_list:
-            if self.compute_dist(curr_pos, prev_pos) < min_distance:
-                satisfy = False
-                break
-        return satisfy
-
-    def random_position(self):
-        while True:
-            pos = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))
-            if self.grid[pos[0], pos[1]] is None and self.min_dist(pos,3):
-                self.prev_pos_list.append(pos)
-                return pos
-
-    def compute_dist(self, pos1, pos2):
-        pos1 = np.array([pos1[0], pos1[1]])
-        pos2 = np.array([pos2[0], pos2[1]])
-        return np.linalg.norm(pos1 - pos2)
-    
-    def observe(self):
-        agent_obs = []
-        for agent in self.agents:
-            agent_obs.append(agent.observe(self))
-        return agent_obs
-
-    def step(self, agent_actions):
-        # One step in the simulation
-        # Gather each agent's chosen action for consensus on movement
-        actions = []
-        self.rewards = np.array([0] * NUM_AGENTS)
-        for i, agent in enumerate(self.agents):
-            if agent.done:
-                continue
-
-            action = agent_actions[i]
-            actions.append((agent, action))
-
-        # Consensus action is for agents taking the same food items
-        consensus_action = {}
-        for food in self.foods:
-            if len(food.carried) > 1:
-                first_id = food.carried[0]
-                consensus_action[food.id] = actions[first_id][1] if all(a[1] == actions[first_id][1] for a in actions if a[0].id in food.carried) else None
-        # print("food taken by multiagent", consensus_action.keys())
-        # Process each agentâ€™s action
-        for agent, action in actions:
-            # If an agent is tied to other agents, i.e., picking the same food.
-            # Consensus action has to be satisfied for all agents to perform action, move or drop food.  Otherwise, these actions have no effect.
-            if agent.carrying_food:
-                if agent.carrying_food.id in consensus_action:
-                    if consensus_action[agent.carrying_food.id] is None:
-                        print(f"Agent {agent.id} couldn't move; consensus required.")
-                        continue
-            if action in ["up", "down", "left", "right"]:
-                delta_pos = {'up': np.array([-1,0]),
-                            'down': np.array([1,0]),
-                            'left': np.array([0,-1]),
-                            'right': np.array([0,1]),}
-                old_agent_position = np.array(agent.position)
-                new_agent_position = old_agent_position + delta_pos[action]
-
-                # Check if the new position is empty and move if it is
-                # Note: This code will iterate all the agents that carry the same food.
-                # The condition not(agent.carrying_food.is_moved) will make sure all agents are iterated only one time
-                # print(f"check {agent.id}")
-                if agent.carrying_food and not(agent.carrying_food.is_moved):
-                    # print(f"agent {agent.id} is not moved")
-                    move = True
-                    new_food_position = agent.carrying_food.position + delta_pos[action]
-                    new_pos_list = [new_food_position]
-                     # the new position of each element has to be unoccupieds
-                    for agent_id in agent.carrying_food.carried:
-                        new_pos_list.append(self.agents[agent_id].position + delta_pos[action])
-
-                    for id,new_pos in enumerate(new_pos_list):
-                        if new_pos[0] < 0 or new_pos[1] < 0 or new_pos[0] > GRID_SIZE-1 or new_pos[1] > GRID_SIZE-1:
-                            # print(f"Bounded, {agent.id}'s move = False, because new_pos number {id}")
-                            move = False                            
-                            break
-
-                        check_grid = self.grid[new_pos[0], new_pos[1]]
-                        if isinstance(check_grid, Agent) and (check_grid.id not in agent.carrying_food.carried) or \
-                            isinstance(check_grid, Food) and (check_grid.id != agent.carrying_food.id):
-                            move = False
-                            # print(f"{agent.id}'s move = False because new_pos number {id}")
-                            # print(f"first condition= {isinstance(check_grid, Agent) and (check_grid.id not in agent.carrying_food.carried)}")
-                            # print(f"second condition= {isinstance(check_grid, Food) and (check_grid.id != agent.carrying_food.id)}")
-                            break
-
-
-                    if move:
-                        # Move the food with the agent
-                        # print(f"{agent.id} moves to {new_agent_position}")
-                        for agent_id in agent.carrying_food.carried:
-                            old_position = self.agents[agent_id].position
-                            new_position = self.agents[agent_id].position + delta_pos[action]
-                            self.agents[agent_id].position = new_position
-                            # print(f"agent {agent_id} moves to {new_position} from {old_position}")
-                        if not(agent.carrying_food.is_moved):
-                            # print(f"{agent.carrying_food.id} moves to {new_food_position}")
-                            agent.carrying_food.position = new_food_position
-                            agent.carrying_food.is_moved = True
-                        loss = (1 + 0.2*min(agent.strength, agent.carrying_food.strength_required))
-                        agent.energy -= loss # When carry food, agent lose more energy due to friction
-
-                elif not(agent.carrying_food):
-                    if new_agent_position[0] < 0 or new_agent_position[1] < 0 or new_agent_position[0] > GRID_SIZE-1 or new_agent_position[1] > GRID_SIZE-1:
-                        continue
-
-                    if self.grid[new_agent_position[0], new_agent_position[1]] is None:
-                        agent.energy -= 1
-                        agent.position += delta_pos[action]
-
-                self.update_grid()
-
-            elif action == "pick_up" and agent.carrying_food is None:
-                for food in self.foods:
-                    if (self.compute_dist(food.position, agent.position) <= np.sqrt(2)) and len(food.carried) == 0:
-                        # If the combined strength satisfies the required strength, the food is picked up sucessfully
-                        if food.strength_required - food.reduced_strength <= agent.strength and not food.carried:
-                            food.carried += food.pre_carried
-                            food.carried.append(agent.id)
-                            for agent_id in food.carried:
-                                self.agents[agent_id].carrying_food = food
-                                self.rewards[agent_id] += pickup_reward
-                            food.pre_carried.clear()
-                            # print(f"Agents {food.carried} picked up food at {food.position}")
-                            break
-
-                        # If food is too heavy, the heaviness is reduced by the strength of the picking agent.
-                        # Other agents can pick up if the combined strength satisfies the required strength
-                        elif food.strength_required - food.reduced_strength > agent.strength and not food.carried:
-                            food.reduced_strength += agent.strength
-                            food.pre_carried.append(agent.id) # agent.id prepares to carry the food.
-
-            elif action == "drop" and agent.carrying_food:
-                # If agent drops food at home
-                if (agent.carrying_food.position[0] in range(HOME_POSITION[0], HOME_POSITION[0] + HOME_SIZE) and 
-                    agent.carrying_food.position[1] in range(HOME_POSITION[1], HOME_POSITION[1] + HOME_SIZE)):
-                    
-                    # Dismiss the dropped food item at home
-                    agent.carrying_food.position = (-2000,-2000)
-                    agent.carrying_food.done = True
-                    self.collected_foods.add(agent.carrying_food.id)
-                    
-                    for agent_id in agent.carrying_food.carried:
-                        self.agents[agent_id].energy += self.agents[agent_id].carrying_food.energy_score # TODO this is wrong another agent has to get energy too
-                        self.rewards[agent_id] += self.agents[agent_id].carrying_food.energy_score * drop_reward # TODO this is wrong another agent has to get energy too
-                        
-                        self.agents[agent_id].carrying_food.carried = []
-                        self.agents[agent_id].carrying_food = None
-
-                    
-                else:
-                    self.rewards[agent.id] += drop_punishment
-                    agent.carrying_food.carried = []
-                    agent.carrying_food = None
-                    
-
-        # All agents have to pick up food at the same time step.
-        for food in self.foods:
-            food.reduced_strength = 0 # clear reduced strenth due to combined strength
-            food.pre_carried.clear() # clear list
-            food.is_moved = False
-
-        # End conditions
-        # End if all food items are collected
-        if len(self.collected_foods) == len(self.foods):
-            print("All food items are collected")
-            self.rewards += np.array([collect_all_reward] * NUM_AGENTS)
-            return self.observe(), np.copy(self.rewards), True, None, None
-
-        # End if any agent runs out of energy
-        if agent.energy <= 0:
-            agent.done = True
-            self.rewards += np.array([energy_punishment] * NUM_AGENTS)
-            return self.observe(), np.copy(self.rewards), True, None, None
-
-        return self.observe(), np.copy(self.rewards), False, None, None
\ No newline at end of file
diff --git a/environment_single.py b/environment_single.py
index 451a857..a590864 100644
--- a/environment_single.py
+++ b/environment_single.py
@@ -13,7 +13,7 @@ from keyboard_control import *
 # Environment Parameters
 GRID_SIZE = 10  # Size of the grid world
 NUM_AGENTS = 1  # Number of agents
-NUM_FOODS = 5  # Number of foods
+NUM_FOODS = 6  # Number of foods
 ENERGY_FACTOR = 20
 HOME_POSITION = (0, 0)  # Coordinates of the home
 HOME_SIZE = 2
@@ -42,10 +42,14 @@ drop_reward_factor = 0.1 # multiplying with energy
 energy_reward_factor = 0.01
 
 class Environment(gym.Env):
-    def __init__(self, truncated=False, torch_order=True):
-        self.truncated = truncated
+    def __init__(self, truncated=False, torch_order=True, num_agents=1):
+        self.num_agents = num_agents
+        self.grid_size = 10
+        self.image_size = 5
+        self.num_channels = 1
         self.torch_order = torch_order
         self.info = {}
+        self.truncated = truncated
         self.image_shape = (NUM_CHANNELS,5,5) if self.torch_order else (5,5,NUM_CHANNELS)
         self.observation_space = spaces.Dict(
             {
@@ -64,11 +68,14 @@ class Environment(gym.Env):
         self.grid = np.full((GRID_SIZE, GRID_SIZE), None)
         self.prev_pos_list = []
         # Initialize agents with uniform attributes
-        self.agents = [EnvAgent(i, self.random_position(), AGENT_STRENGTH, AGENT_ENERGY) for i in range(NUM_AGENTS)]
-        for agent in self.agents:
+        self.agent_maps = [EnvAgent(i, self.random_position(), AGENT_STRENGTH, AGENT_ENERGY) for i in range(NUM_AGENTS)]
+        for agent in self.agent_maps:
             self.grid[agent.position[0], agent.position[1]] = agent
-        # self.foods = [Food(self.random_position(), food_id+2, food_id) for food_id in range(NUM_FOODS)]
-        self.foods = [Food(self.random_position(), 1, food_id) for food_id in range(NUM_FOODS)]
+        #  position, food_type, id)
+        self.foods = [Food(position=self.random_position(), 
+                            food_type = food_id+1,
+                            id=food_id) for food_id in range(NUM_FOODS)
+                            ]
         for food in self.foods:
             self.grid[food.position[0], food.position[1]] = food
 
@@ -81,7 +88,7 @@ class Environment(gym.Env):
         Update grid position after agents move
         '''
         self.grid = np.full((GRID_SIZE, GRID_SIZE), None)
-        for agent in self.agents:
+        for agent in self.agent_maps:
             if not(agent.done): # If agent is alive
                 self.grid[agent.position[0], agent.position[1]] = agent
         for food in self.foods:
@@ -124,12 +131,12 @@ class Environment(gym.Env):
         agent_obs = []
         agent_loc = []
         if NUM_AGENTS==1:
-            image = self.agents[0].observe(self)
+            image = self.agent_maps[0].observe(self)
             if self.torch_order:
                 image = np.transpose(image, (2,0,1))
-            return {"image": image, "location": self.agents[0].position}
+            return {"image": image, "location": self.agent_maps[0].position}
         else:
-            for agent in self.agents:
+            for agent in self.agent_maps:
                 image = agent.observe(self)
                 if self.torch_order:
                     image = np.transpose(image, (2,0,1))
@@ -167,7 +174,7 @@ class Environment(gym.Env):
         # Gather each agent's chosen action for consensus on movement
         actions = []
         self.rewards = np.zeros((NUM_AGENTS))
-        for i, agent in enumerate(self.agents):
+        for i, agent in enumerate(self.agent_maps):
             # End if any agent runs out of energy
             if agent.energy <= 0:
                 agent.done = True
@@ -226,7 +233,7 @@ class Environment(gym.Env):
                     new_pos_list = [new_food_position]
                      # the new position of each element has to be unoccupieds
                     for agent_id in agent.carrying_food.carried:
-                        new_pos_list.append(self.agents[agent_id].position + delta_pos[action])
+                        new_pos_list.append(self.agent_maps[agent_id].position + delta_pos[action])
 
                     for id,new_pos in enumerate(new_pos_list):
                         if new_pos[0] < 0 or new_pos[1] < 0 or new_pos[0] > GRID_SIZE-1 or new_pos[1] > GRID_SIZE-1:
@@ -246,11 +253,11 @@ class Environment(gym.Env):
 
                     if move:
                         for agent_id in agent.carrying_food.carried:
-                            old_position = self.agents[agent_id].position
-                            new_position = self.agents[agent_id].position + delta_pos[action]
-                            self.agents[agent_id].position = new_position
-                            loss = 0.2*min(self.agents[agent_id].strength, self.agents[agent_id].carrying_food.strength_required)
-                            self.agents[agent_id].energy -= loss # When carry food, agent lose more energy due to friction
+                            old_position = self.agent_maps[agent_id].position
+                            new_position = self.agent_maps[agent_id].position + delta_pos[action]
+                            self.agent_maps[agent_id].position = new_position
+                            loss = 0.2*min(self.agent_maps[agent_id].strength, self.agent_maps[agent_id].carrying_food.strength_required)
+                            self.agent_maps[agent_id].energy -= loss # When carry food, agent lose more energy due to friction
                             # step_reward
                             # old_home_dist = self.compute_dist(old_position, HOME_POSITION)
                             # new_home_dist = self.compute_dist(new_position, HOME_POSITION)
@@ -280,7 +287,7 @@ class Environment(gym.Env):
                             food.carried.append(agent.id)
                             for agent_id in food.carried:
                                 # step_reward
-                                self.agents[agent_id].carrying_food = food
+                                self.agent_maps[agent_id].carrying_food = food
                                 self.rewards[agent_id] += pickup_reward
                             food.pre_carried.clear()
                             # print(f"Agents {food.carried} picked up food at {food.position}")
@@ -307,11 +314,11 @@ class Environment(gym.Env):
                     
                     for agent_id in agent.carrying_food.carried:
                         # step_reward
-                        self.agents[agent_id].energy += self.agents[agent_id].carrying_food.energy_score # TODO this is wrong another agent has to get energy too
-                        self.rewards[agent_id] += self.agents[agent_id].carrying_food.energy_score * drop_reward_factor # TODO this is wrong another agent has to get energy too
+                        self.agent_maps[agent_id].energy += self.agent_maps[agent_id].carrying_food.energy_score # TODO this is wrong another agent has to get energy too
+                        self.rewards[agent_id] += self.agent_maps[agent_id].carrying_food.energy_score * drop_reward_factor # TODO this is wrong another agent has to get energy too
                         
-                        self.agents[agent_id].carrying_food.carried = []
-                        self.agents[agent_id].carrying_food = None
+                        self.agent_maps[agent_id].carrying_food.carried = []
+                        self.agent_maps[agent_id].carrying_food = None
 
                     
                 else:
@@ -345,7 +352,7 @@ class Environment(gym.Env):
             # terminal_reward
             self.rewards += np.array([collect_all_reward] * NUM_AGENTS)
             self.done = True
-            for agent in self.agents:
+            for agent in self.agent_maps:
                 self.rewards[agent.id] += energy_reward_factor * agent.energy
             # return self.observe(), np.copy(self.rewards)[0], True,  self.truncated, self.info
         self.cumulative_reward += np.sum(np.copy(self.rewards))
@@ -417,28 +424,27 @@ class EnvAgent:
         return env.message[0,:] #TODO Use neural network to send message
 
 class Food:
-    def __init__(self, position, strength_required, id):
+    def __init__(self, position, food_type, id):
+        self.type_to_strength_map = {
+                                    1:3, # Spinach
+                                    2:3,  # Watermelon
+                                    3:3, # Strawberry
+                                    4:6,  # Chicken
+                                    5:6,  # Pig
+                                    6:6 # Cattle
+                                    }
         self.position = position
-        self.strength_required = strength_required
+        self.food_type = food_type
+        self.strength_required = self.type_to_strength_map[food_type]
         self.carried = [] # keep all agents that already picked up this food
         self.pre_carried = [] # keep all agents that try to pick up this food, no need to be successful
-        self.attribute = self.generate_attributes(strength_required)
-        self.energy_score = ENERGY_FACTOR * strength_required
+        self.attribute = self.generate_attributes(food_type)
+        self.energy_score = ENERGY_FACTOR * self.strength_required
         self.id = id
         self.done = False
         self.reduced_strength = 0
 
-    def generate_attributes(self, strength_required):
-        # Return unique attributes based on the food's strength requirement
-        # attribute_mapping = {
-        #     1: [0, 255, 0, 0], # Spinach
-        #     2: [255, 0, 0, 0], # Watermelon
-        #     3: [186, 11, 11, 0], # Strawberry
-        #     4: [255, 132, 185, 0], # Chicken
-        #     5: [255, 185, 235, 0], # Pig
-        #     6: [148, 76, 14, 0], # Cattle
-
-        # }
+    def generate_attributes(self, food_type):
         attribute_mapping = {
             1: [10], # Spinach
             2: [20], # Watermelon
@@ -448,4 +454,4 @@ class Food:
             6: [60], # Cattle
 
         }
-        return np.array(attribute_mapping.get(strength_required, [1, 1, 1, 1]))
\ No newline at end of file
+        return np.array(attribute_mapping.get(food_type, [1, 1, 1, 1]))
\ No newline at end of file
diff --git a/test_ppo_lstm.py b/test_ppo_lstm.py
deleted file mode 100644
index dcdc76a..0000000
--- a/test_ppo_lstm.py
+++ /dev/null
@@ -1,140 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_atari_lstmpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import tyro
-from torch.distributions.categorical import Categorical
-from torch.utils.tensorboard import SummaryWriter
-
-from stable_baselines3.common.atari_wrappers import (  # isort:skip
-    ClipRewardEnv,
-    EpisodicLifeEnv,
-    FireResetEnv,
-    MaxAndSkipEnv,
-    NoopResetEnv,
-)
-
-from nets import *
-from constants import *
-from keyboard_control import *
-from environment import *
-from buffer import *
-from train_ppo_lstm import *
-
-
-@dataclass
-class Args:
-    ckpt_path = "checkpoints/ppo/final_model.pt"
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    seed: int = 1
-    torch_deterministic: bool = True
-    cuda: bool = True
-    track: bool = True
-    wandb_project_name: str = "PPO Foraging Game"
-    wandb_entity: str = "maytusp"
-    capture_video: bool = False
-    video_save_dir = "vids/ppo"
-    visualize = True
-
-    # Algorithm specific arguments
-    env_id: str = "Foraging-Single-v1"
-    total_episodes: int = 100
-    num_channels = 4
-    num_obs_grid = 5
-
-
-
-if __name__ == "__main__":
-    env = Environment()
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-
-    if args.visualize:
-        from visualize import *
-        from moviepy.editor import *
-
-    # if args.track:
-    #     import wandb
-
-    #     wandb.init(
-    #         project=args.wandb_project_name,
-    #         entity=args.wandb_entity,
-    #         sync_tensorboard=True,
-    #         config=vars(args),
-    #         name=run_name,
-    #         monitor_gym=True,
-    #         save_code=True,
-    #     )
-    # writer = SummaryWriter(f"runs/{run_name}")
-    # writer.add_text(
-    #     "hyperparameters",
-    #     "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    # )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    envs = gym.vector.SyncVectorEnv(
-    [make_env(args.env_id, i, args.capture_video, run_name) for i in range(1)],
-    )
-
-    agent = PPOLSTMAgent(envs).to(device)
-    agent.load_state_dict(torch.load(args.ckpt_path, map_location=device))
-    agent.eval()
-
-
-    # TRY NOT TO MODIFY: start the game
-    global_step = 0
-    start_time = time.time()
-    next_lstm_state = (
-        torch.zeros(agent.lstm.num_layers, 1, agent.lstm.hidden_size).to(device),
-        torch.zeros(agent.lstm.num_layers, 1, agent.lstm.hidden_size).to(device),
-    )
-    average_sr = 0
-    for episode_id in range(1, args.total_episodes + 1):
-        
-        next_obs_dict, _ = envs.reset(seed=args.seed)
-        next_obs, next_locs = next_obs_dict["image"], next_obs_dict["location"]
-        next_obs = torch.Tensor(next_obs).to(device)
-        next_locs = torch.Tensor(next_locs).to(device)
-        next_done = torch.zeros((1)).to(device)
-        initial_lstm_state = (next_lstm_state[0].clone(), next_lstm_state[1].clone())
-        returns = 0
-        ep_step = 0
-        frames = []
-        while not next_done:
-            if args.visualize:
-                frame = visualize_environment(envs.envs[0], ep_step)
-                frames.append(frame.transpose((1, 0, 2)))
-            with torch.no_grad():
-                action, logprob, _, value, next_lstm_state = agent.get_action_and_value(next_obs, next_lstm_state, next_done)
-            env_action = action.cpu().numpy()
-            next_obs_dict, reward, terminations, truncations, infos = envs.step(env_action)
-            next_obs, next_locs = next_obs_dict["image"], next_obs_dict["location"]
-            next_done = np.logical_or(terminations, truncations)
-            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)
-            returns += np.mean(reward)
-            ep_step+=1
-        if args.visualize: # and returns > 5:
-            os.makedirs(args.video_save_dir, exist_ok=True)
-            clip = ImageSequenceClip(frames, fps=5)
-            clip.write_videofile(os.path.join(args.video_save_dir, f"ep_{episode_id}_return={returns}.mp4"), codec="libx264")
-        print(f"Total Reward: {returns}")
-        if returns > 0:
-            average_sr += 1
-
-    print(f"Average SR: {average_sr / args.total_episodes}")
-    envs.close()
-    # writer.close()
\ No newline at end of file
diff --git a/train_marl_ppo.py b/train_marl_ppo.py
index ff217b9..7a4e3aa 100644
--- a/train_marl_ppo.py
+++ b/train_marl_ppo.py
@@ -27,7 +27,7 @@ from utils import *
 
 @dataclass
 class Args:
-    save_dir = "checkpoints/ippo"
+    save_dir = "checkpoints/ppo"
     os.makedirs(save_dir, exist_ok=True)
     save_frequency = 10000
     exp_name: str = os.path.basename(__file__)[: -len(".py")]
@@ -40,7 +40,7 @@ class Args:
     """if toggled, cuda will be enabled by default"""
     track: bool = True
     """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "IPPO MA Foraging Game"
+    wandb_project_name: str = "IPPO Foraging"
     """the wandb's project name"""
     wandb_entity: str = "maytusp"
     """the entity (team) of wandb's project"""
@@ -50,11 +50,12 @@ class Args:
     # Algorithm specific arguments
     env_id: str = "Foraging-Single-v1"
     """the id of the environment"""
-    total_timesteps: int = 10000000
+    total_timesteps: int = 30000000
     """total timesteps of the experiments"""
     learning_rate: float = 2.5e-4
     """the learning rate of the optimizer"""
     num_envs: int = 8
+    num_agents: int = 2
     """the number of parallel game environments"""
     num_steps: int = 128
     """the number of steps to run in each environment per policy rollout"""
@@ -192,7 +193,7 @@ if __name__ == "__main__":
 
     device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
 
-    env = Environment()
+    env = Environment(num_agents=args.num_agents)
     grid_size = (env.image_size, env.image_size)
     num_channels = env.num_channels
     num_agents = len(env.possible_agents)
diff --git a/train_marl_ppo_old.py b/train_marl_ppo_old.py
deleted file mode 100644
index dc6804a..0000000
--- a/train_marl_ppo_old.py
+++ /dev/null
@@ -1,334 +0,0 @@
-"""Basic code which shows what it's like to run PPO on the Pistonball env using the parallel API, this code is inspired by CleanRL.
-
-This code is exceedingly basic, with no logging or weights saving.
-The intention was for users to have a (relatively clean) ~200 line file to refer to when they want to design their own learning algorithm.
-
-Author: Jet (https://github.com/jjshoots)
-"""
-
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.optim as optim
-from torch.distributions.categorical import Categorical
-import supersuit as ss
-
-from nets import *
-from constants import *
-from keyboard_control import *
-from environment import *
-from buffer import *
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-class PPOLSTMAgent(nn.Module):
-    def __init__(self, num_actions):
-        super().__init__()
-        self.nonlinear = nn.Sequential(nn.Flatten(), # (1,5,5) to (25)
-                                        layer_init(nn.Linear(25, 256)), 
-                                        nn.ReLU(),
-                                        layer_init(nn.Linear(256, 256)),
-                                        nn.ReLU(),
-                                        layer_init(nn.Linear(256, 256)),
-                                        nn.ReLU(),
-                                        layer_init(nn.Linear(256, 256)),
-                                        nn.ReLU(),
-                                        )       
-        self.lstm = nn.LSTM(256, 128)
-        for name, param in self.lstm.named_parameters():
-            if "bias" in name:
-                nn.init.constant_(param, 0)
-            elif "weight" in name:
-                nn.init.orthogonal_(param, 1.0)
-        self.actor = layer_init(nn.Linear(128, num_actions), std=0.01)
-        self.critic = layer_init(nn.Linear(128, 1), std=1)
-
-    def get_states(self, x, lstm_state, done):
-        # hidden = self.nonlinear(self.network(x / 255.0)) # CNN
-        hidden = self.nonlinear(x / 255.0) # MLP
-
-        # LSTM logic
-        batch_size = lstm_state[0].shape[1]
-
-        hidden = hidden.reshape((-1, batch_size, self.lstm.input_size))
-        done = done.reshape((-1, batch_size))
-        new_hidden = []
-        for h, d in zip(hidden, done):
-            d_adjust =  (1.0 - d).view(1, -1, 1)
-            h, lstm_state = self.lstm(
-                h.unsqueeze(0),
-                (
-                    d_adjust * lstm_state[0],
-                    d_adjust * lstm_state[1],
-                ),
-            )
-            new_hidden += [h]
-        new_hidden = torch.flatten(torch.cat(new_hidden), 0, 1)
-        return new_hidden, lstm_state
-
-    def get_value(self, x, lstm_state, done):
-        hidden, _ = self.get_states(x, lstm_state, done)
-        return self.critic(hidden)
-
-    def get_action_and_value(self, x, lstm_state, done, action=None):
-        hidden, lstm_state = self.get_states(x, lstm_state, done)
-        logits = self.actor(hidden)
-        probs = Categorical(logits=logits)
-        if action is None:
-            action = probs.sample()
-        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden), lstm_state
-
-
-def batchify_obs(obs_dict, device):
-    #TODO next_obs, next_locs, next_eners = next_obs_dict["image"], next_obs_dict["location"], next_obs_dict["energy"]
-    """Converts PZ style observations to batch of torch arrays."""
-    # convert to list of np arrays
-
-    obs = np.array([a for a in obs_dict['image']])
-    locs = np.array([a for a in obs_dict['location']])
-    eners = np.array([a for a in obs_dict['energy']])
-
-    # convert to torch
-    obs = torch.tensor(obs).to(device)
-    locs = torch.tensor(locs).to(device)
-    eners = torch.tensor(eners).to(device)
-    return obs, locs, eners
-
-
-
-def unbatchify(x, num_envs, env):
-    """Converts np array to PZ style arguments."""
-    x = x.cpu().numpy()
-
-    # x = {i: x[i] for i in range(num_envs)}
-    x = {i:{j:0 for j in range(2)} for i in range(8)}
-
-    return x
-
-
-if __name__ == "__main__":
-    """ALGO PARAMS"""
-    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    num_envs = 16
-    ent_coef = 0.1
-    vf_coef = 0.1
-    clip_coef = 0.1
-    gamma = 0.99
-    batch_size = 64
-    max_cycles = 125
-    total_episodes = 2
-    update_epochs=4
-    seed=1
-
-    """ ENV SETUP """
-    env = Environment()
-    grid_size = (env.image_size, env.image_size)
-    num_channels = env.num_channels
-    num_agents = len(env.possible_agents)
-    num_actions = env.action_space(env.possible_agents[0]).n
-    observation_size = env.observation_space(env.possible_agents[0]).shape
-
-    # Vectorise env
-    # env = ss.clip_reward_v0(env, lower_bound=-1, upper_bound=1)
-    env = ss.pettingzoo_env_to_vec_env_v1(env)
-    env = ss.concat_vec_envs_v1(env, num_envs // num_agents, num_cpus=0, base_class="gymnasium")
-    
-    
-    
-
-    """ LEARNER SETUP """
-    agent = PPOLSTMAgent(num_actions=num_actions).to(device)
-    optimizer = optim.Adam(agent.parameters(), lr=0.001, eps=1e-5)
-
-    """ ALGO LOGIC: EPISODE STORAGE"""
-    end_step = 0
-    total_episodic_return = 0
-    rb_obs = torch.zeros((max_cycles, num_envs, num_channels, *grid_size)).to(device)
-    rb_actions = torch.zeros((max_cycles, num_envs)).to(device)
-    rb_logprobs = torch.zeros((max_cycles, num_envs)).to(device)
-    rb_rewards = torch.zeros((max_cycles, num_envs)).to(device)
-    rb_terms = torch.zeros((max_cycles, num_envs)).to(device)
-    rb_values = torch.zeros((max_cycles, num_envs)).to(device)
-
-    # TRY NOT TO MODIFY: start the game
-    global_step = 0
-    start_time = time.time()
-    terms = torch.zeros(num_envs).to(device)
-    next_lstm_state = (
-        torch.zeros(agent.lstm.num_layers, num_envs, agent.lstm.hidden_size).to(device),
-        torch.zeros(agent.lstm.num_layers, num_envs, agent.lstm.hidden_size).to(device),
-    )  # hidden and cell states (see https://youtu.be/8HyCNIVRbSU)
-
-    """ TRAINING LOGIC """
-    # train for n number of episodes
-    for episode in range(total_episodes):
-        initial_lstm_state = (next_lstm_state[0].clone(), next_lstm_state[1].clone())
-        # collect an episode
-        with torch.no_grad():
-            # collect observations and convert to batch of torch tensors
-            next_obs_dict, _ = env.reset()
-            
-            # reset the episodic return
-            total_episodic_return = 0
-
-            # each episode has num_steps
-            for step in range(0, max_cycles):
-                # rollover the observation
-                obs, locs, eners = batchify_obs(next_obs_dict, device)
-                actions, logprobs, _, values, next_lstm_state = agent.get_action_and_value(obs, next_lstm_state, terms)
-
-                # get action from the agent
-                # execute the environment and log data
-                # next_obs_dict, rewards, terms, truncs, infos = env.step(
-                #     unbatchify(actions, num_envs, env)
-                # )
-                next_obs_dict, rewards, terms, truncs, infos = env.step(
-                    actions.cpu().numpy()
-                )
-
-                terms = torch.tensor(terms).to(device)
-                rewards = torch.Tensor(rewards).to(device)
-
-                # add to episode storage
-                rb_obs[step] = obs
-                rb_rewards[step] = rewards
-                rb_terms[step] = terms
-                rb_actions[step] = actions
-                rb_logprobs[step] = logprobs
-                rb_values[step] = values.flatten()
-
-
-                
-                # compute episodic return
-                total_episodic_return += rb_rewards[step].cpu().numpy()
-
-                # if we reach termination or truncation, end
-                if any([done for done in terms]):
-                    end_step = step
-                    break
-
-        # bootstrap value if not done
-        with torch.no_grad():
-            rb_advantages = torch.zeros_like(rb_rewards).to(device)
-            for t in reversed(range(end_step)):
-                delta = (
-                    rb_rewards[t]
-                    + gamma * rb_values[t + 1] * rb_terms[t + 1]
-                    - rb_values[t]
-                )
-                rb_advantages[t] = delta + gamma * gamma * rb_advantages[t + 1]
-            rb_returns = rb_advantages + rb_values
-        print("end_step", end_step)
-        print("rb_obs", rb_obs.shape)
-        # convert our episodes to batch of individual transitions
-        b_obs = torch.flatten(rb_obs[:end_step], start_dim=0, end_dim=1)
-        b_logprobs = torch.flatten(rb_logprobs[:end_step], start_dim=0, end_dim=1)
-        b_actions = torch.flatten(rb_actions[:end_step], start_dim=0, end_dim=1)
-        b_returns = torch.flatten(rb_returns[:end_step], start_dim=0, end_dim=1)
-        b_values = torch.flatten(rb_values[:end_step], start_dim=0, end_dim=1)
-        b_advantages = torch.flatten(rb_advantages[:end_step], start_dim=0, end_dim=1)
-        b_terms = torch.flatten(rb_terms[:end_step], start_dim=0, end_dim=1)
-
-        # Optimizing the policy and value network
-        print("b_obs", b_obs.shape)
-        b_index = np.arange(len(b_obs))
-
-
-        # Optimizing the policy and value network
-        b_inds = np.arange(batch_size)
-        clipfracs = []
-        for epoch in range(update_epochs):
-            np.random.shuffle(b_inds)
-            for start in range(0, batch_size, args.minibatch_size):
-                end = start + args.minibatch_size
-                mb_inds = b_inds[start:end]
-
-                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])
-                logratio = newlogprob - b_logprobs[mb_inds]
-                ratio = logratio.exp()
-
-                with torch.no_grad():
-                    # calculate approx_kl http://joschu.net/blog/kl-approx.html
-                    old_approx_kl = (-logratio).mean()
-                    approx_kl = ((ratio - 1) - logratio).mean()
-                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]
-
-                mb_advantages = b_advantages[mb_inds]
-                if args.norm_adv:
-                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
-
-                # Policy loss
-                pg_loss1 = -mb_advantages * ratio
-                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
-                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
-
-                # Value loss
-                newvalue = newvalue.view(-1)
-                if args.clip_vloss:
-                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
-                    v_clipped = b_values[mb_inds] + torch.clamp(
-                        newvalue - b_values[mb_inds],
-                        -args.clip_coef,
-                        args.clip_coef,
-                    )
-                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
-                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
-                    v_loss = 0.5 * v_loss_max.mean()
-                else:
-                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
-
-                entropy_loss = entropy.mean()
-                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
-
-                optimizer.zero_grad()
-                loss.backward()
-                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
-                optimizer.step()
-
-            if args.target_kl is not None:
-                if approx_kl > args.target_kl:
-                    break
-
-        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
-        var_y = np.var(y_true)
-        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
-        writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
-        writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
-        writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
-        writer.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
-        writer.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
-        writer.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
-        writer.add_scalar("losses/explained_variance", explained_var, global_step)
-        print("SPS:", int(global_step / (time.time() - start_time)))
-        writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-
-    envs.close()
-    writer.close()
-
-    # """ RENDER THE POLICY """
-    # env = pistonball_v6.parallel_env(render_mode="human", continuous=False)
-    # env = color_reduction_v0(env)
-    # env = resize_v1(env, 64, 64)
-    # env = frame_stack_v1(env, stack_size=4)
-
-    # agent.eval()
-
-    # with torch.no_grad():
-    #     # render 5 episodes out
-    #     for episode in range(5):
-    #         obs, infos = env.reset(seed=None)
-    #         obs = batchify_obs(obs, device)
-    #         terms = [False]
-    #         truncs = [False]
-    #         while not any(terms) and not any(truncs):
-    #             actions, logprobs, _, values = agent.get_action_and_value(obs)
-    #             obs, rewards, terms, truncs, infos = env.step(unbatchify(actions, env))
-    #             obs = batchify_obs(obs, device)
-    #             terms = [terms[a] for a in terms]
-    #             truncs = [truncs[a] for a in truncs]
\ No newline at end of file
diff --git a/train_ppo_lstm.py b/train_ppo_lstm.py
deleted file mode 100644
index bb1a687..0000000
--- a/train_ppo_lstm.py
+++ /dev/null
@@ -1,400 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_atari_lstmpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import tyro
-from torch.distributions.categorical import Categorical
-from torch.utils.tensorboard import SummaryWriter
-
-from stable_baselines3.common.atari_wrappers import (  # isort:skip
-    ClipRewardEnv,
-    EpisodicLifeEnv,
-    FireResetEnv,
-    MaxAndSkipEnv,
-    NoopResetEnv,
-)
-
-from nets import *
-from constants import *
-from keyboard_control import *
-from environment import *
-from buffer import *
-
-
-@dataclass
-class Args:
-    save_dir = "checkpoints/ppo"
-    os.makedirs(save_dir, exist_ok=True)
-    save_frequency = 10000
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "PPO-MLP Foraging Game"
-    """the wandb's project name"""
-    wandb_entity: str = "maytusp"
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Foraging-Single-v1"
-    """the id of the environment"""
-    total_timesteps: int = 10000000
-    """total timesteps of the experiments"""
-    learning_rate: float = 2.5e-4
-    """the learning rate of the optimizer"""
-    num_envs: int = 8
-    """the number of parallel game environments"""
-    num_steps: int = 128
-    """the number of steps to run in each environment per policy rollout"""
-    anneal_lr: bool = True
-    """Toggle learning rate annealing for policy and value networks"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    gae_lambda: float = 0.95
-    """the lambda for the general advantage estimation"""
-    num_minibatches: int = 4
-    """the number of mini-batches"""
-    update_epochs: int = 4
-    """the K epochs to update the policy"""
-    norm_adv: bool = True
-    """Toggles advantages normalization"""
-    clip_coef: float = 0.1
-    """the surrogate clipping coefficient"""
-    clip_vloss: bool = True
-    """Toggles whether or not to use a clipped loss for the value function, as per the paper."""
-    ent_coef: float = 0.01
-    """coefficient of the entropy"""
-    vf_coef: float = 0.5
-    """coefficient of the value function"""
-    max_grad_norm: float = 0.5
-    """the maximum norm for the gradient clipping"""
-    target_kl: float = None
-    """number of action"""
-    num_channels = 1
-    """number of channels in observation (non rgb case)"""
-    num_obs_grid = 5
-    """number of observation grid"""
-
-    # to be filled in runtime
-    batch_size: int = 0
-    """the batch size (computed in runtime)"""
-    minibatch_size: int = 0
-    """the mini-batch size (computed in runtime)"""
-    num_iterations: int = 0
-    """the number of iterations (computed in runtime)"""
-
-
-def make_env(env_id, idx, capture_video, run_name):
-    def thunk():
-        env = Environment()
-        env = ClipRewardEnv(env)
-        return env
-
-    return thunk
-
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-
-class PPOLSTMAgent(nn.Module):
-    def __init__(self, envs):
-        super().__init__()
-        # self.network = nn.Sequential(
-        #     layer_init(nn.Conv2d(4, 32, 2, stride=1)),
-        #     nn.ReLU(),
-        #     layer_init(nn.Conv2d(32, 64, 2, stride=1)),
-        #     nn.ReLU(),
-        #     layer_init(nn.Conv2d(64, 64, 2, stride=1)),
-        #     nn.ReLU(),
-        #     nn.Flatten()
-        # )
-        # feat = self.network(torch.zeros((1,1,5,5)))
-        # feat_dim = feat.shape[1]
-        self.nonlinear = nn.Sequential(nn.Flatten(), # (1,5,5) to (25)
-                                        layer_init(nn.Linear(25, 256)), 
-                                        nn.ReLU(),
-                                        layer_init(nn.Linear(256, 256)),
-                                        nn.ReLU(),
-                                        layer_init(nn.Linear(256, 256)),
-                                        nn.ReLU(),
-                                        layer_init(nn.Linear(256, 256)),
-                                        nn.ReLU(),
-                                        )       
-        self.lstm = nn.LSTM(256, 128)
-        for name, param in self.lstm.named_parameters():
-            if "bias" in name:
-                nn.init.constant_(param, 0)
-            elif "weight" in name:
-                nn.init.orthogonal_(param, 1.0)
-        self.actor = layer_init(nn.Linear(128, envs.single_action_space.n), std=0.01)
-        self.critic = layer_init(nn.Linear(128, 1), std=1)
-
-    def get_states(self, x, lstm_state, done):
-        # hidden = self.nonlinear(self.network(x / 255.0)) # CNN
-        hidden = self.nonlinear(x / 255.0) # MLP
-
-        # LSTM logic
-        batch_size = lstm_state[0].shape[1]
-        hidden = hidden.reshape((-1, batch_size, self.lstm.input_size))
-        done = done.reshape((-1, batch_size))
-        new_hidden = []
-        for h, d in zip(hidden, done):
-            h, lstm_state = self.lstm(
-                h.unsqueeze(0),
-                (
-                    (1.0 - d).view(1, -1, 1) * lstm_state[0],
-                    (1.0 - d).view(1, -1, 1) * lstm_state[1],
-                ),
-            )
-            new_hidden += [h]
-        new_hidden = torch.flatten(torch.cat(new_hidden), 0, 1)
-        return new_hidden, lstm_state
-
-    def get_value(self, x, lstm_state, done):
-        hidden, _ = self.get_states(x, lstm_state, done)
-        return self.critic(hidden)
-
-    def get_action_and_value(self, x, lstm_state, done, action=None):
-        hidden, lstm_state = self.get_states(x, lstm_state, done)
-        logits = self.actor(hidden)
-        probs = Categorical(logits=logits)
-        if action is None:
-            action = probs.sample()
-        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden), lstm_state
-
-
-if __name__ == "__main__":
-    args = tyro.cli(Args)
-    args.batch_size = int(args.num_envs * args.num_steps)
-    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-    args.num_iterations = args.total_timesteps // args.batch_size
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv(
-        [make_env(args.env_id, i, args.capture_video, run_name) for i in range(args.num_envs)],
-    )
-    assert isinstance(envs.single_action_space, gym.spaces.Discrete), "only discrete action space is supported"
-    print("envs.single_observation_space", envs.single_observation_space)
-    print("envs.single_action_space.n", envs.single_action_space.n)
-    agent = PPOLSTMAgent(envs).to(device)
-    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)
-
-    # ALGO Logic: Storage setup
-    obs = torch.zeros((args.num_steps, args.num_envs, args.num_channels, args.num_obs_grid, args.num_obs_grid)).to(device)
-    locs = torch.zeros((args.num_steps, args.num_envs, 2)).to(device)
-    actions = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    values = torch.zeros((args.num_steps, args.num_envs)).to(device)
-
-    # TRY NOT TO MODIFY: start the game
-    global_step = 0
-    start_time = time.time()
-    next_obs_dict, _ = envs.reset(seed=args.seed)
-    next_obs, next_locs = next_obs_dict["image"], next_obs_dict["location"]
-    next_obs = torch.Tensor(next_obs).to(device)
-    next_locs = torch.Tensor(next_locs).to(device)
-    next_done = torch.zeros(args.num_envs).to(device)
-    next_lstm_state = (
-        torch.zeros(agent.lstm.num_layers, args.num_envs, agent.lstm.hidden_size).to(device),
-        torch.zeros(agent.lstm.num_layers, args.num_envs, agent.lstm.hidden_size).to(device),
-    )  # hidden and cell states (see https://youtu.be/8HyCNIVRbSU)
-
-    for iteration in range(1, args.num_iterations + 1):
-        initial_lstm_state = (next_lstm_state[0].clone(), next_lstm_state[1].clone())
-        # Annealing the rate if instructed to do so.
-        if args.anneal_lr:
-            frac = 1.0 - (iteration - 1.0) / args.num_iterations
-            lrnow = frac * args.learning_rate
-            optimizer.param_groups[0]["lr"] = lrnow
-
-        for step in range(0, args.num_steps):
-            global_step += args.num_envs
-            obs[step] = next_obs
-            dones[step] = next_done
-
-            # ALGO LOGIC: action logic
-            with torch.no_grad():
-                action, logprob, _, value, next_lstm_state = agent.get_action_and_value(next_obs, next_lstm_state, next_done)
-                values[step] = value.flatten()
-            actions[step] = action
-            logprobs[step] = logprob
-
-            # TRY NOT TO MODIFY: execute the game and log data.
-            env_action = action.cpu().numpy()
-            next_obs_dict, reward, terminations, truncations, infos = envs.step(env_action)
-            next_obs, next_locs = next_obs_dict["image"], next_obs_dict["location"]
-            next_done = np.logical_or(terminations, truncations)
-            rewards[step] = torch.tensor(reward).to(device).view(-1)
-            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)
-
-            if (global_step // args.num_envs) % args.save_frequency == 0:  # Adjust `save_frequency` as needed
-                save_path = os.path.join(args.save_dir, f"model_step_{global_step}.pt")
-                torch.save(agent.state_dict(), save_path)
-                print(f"Model saved to {save_path}")
-            
-            if "final_info" in infos:
-                for info in infos["final_info"]:
-                    if info and "episode" in info:
-                        # print(f"global_step={global_step}, episodic_return={info['episode']['r']}")
-                        writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                        writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-
-        # bootstrap value if not done
-        with torch.no_grad():
-            next_value = agent.get_value(
-                next_obs,
-                next_lstm_state,
-                next_done,
-            ).reshape(1, -1)
-            advantages = torch.zeros_like(rewards).to(device)
-            lastgaelam = 0
-            for t in reversed(range(args.num_steps)):
-                if t == args.num_steps - 1:
-                    nextnonterminal = 1.0 - next_done
-                    nextvalues = next_value
-                else:
-                    nextnonterminal = 1.0 - dones[t + 1]
-                    nextvalues = values[t + 1]
-                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]
-                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam
-            returns = advantages + values
-
-        # flatten the batch
-        b_obs = obs.reshape((-1,) + envs.single_observation_space['image'].shape)
-        b_logprobs = logprobs.reshape(-1)
-        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
-        b_dones = dones.reshape(-1)
-        b_advantages = advantages.reshape(-1)
-        b_returns = returns.reshape(-1)
-        b_values = values.reshape(-1)
-
-        # Optimizing the policy and value network
-        assert args.num_envs % args.num_minibatches == 0
-        envsperbatch = args.num_envs // args.num_minibatches
-        envinds = np.arange(args.num_envs)
-        flatinds = np.arange(args.batch_size).reshape(args.num_steps, args.num_envs)
-        clipfracs = []
-        for epoch in range(args.update_epochs):
-            np.random.shuffle(envinds)
-            for start in range(0, args.num_envs, envsperbatch):
-                end = start + envsperbatch
-                mbenvinds = envinds[start:end]
-                mb_inds = flatinds[:, mbenvinds].ravel()  # be really careful about the index
-
-                _, newlogprob, entropy, newvalue, _ = agent.get_action_and_value(
-                    b_obs[mb_inds],
-                    (initial_lstm_state[0][:, mbenvinds], initial_lstm_state[1][:, mbenvinds]),
-                    b_dones[mb_inds],
-                    b_actions.long()[mb_inds],
-                )
-                logratio = newlogprob - b_logprobs[mb_inds]
-                ratio = logratio.exp()
-
-                with torch.no_grad():
-                    # calculate approx_kl http://joschu.net/blog/kl-approx.html
-                    old_approx_kl = (-logratio).mean()
-                    approx_kl = ((ratio - 1) - logratio).mean()
-                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]
-
-                mb_advantages = b_advantages[mb_inds]
-                if args.norm_adv:
-                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
-
-                # Policy loss
-                pg_loss1 = -mb_advantages * ratio
-                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
-                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
-
-                # Value loss
-                newvalue = newvalue.view(-1)
-                if args.clip_vloss:
-                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
-                    v_clipped = b_values[mb_inds] + torch.clamp(
-                        newvalue - b_values[mb_inds],
-                        -args.clip_coef,
-                        args.clip_coef,
-                    )
-                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
-                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
-                    v_loss = 0.5 * v_loss_max.mean()
-                else:
-                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
-
-                entropy_loss = entropy.mean()
-                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
-
-                optimizer.zero_grad()
-                loss.backward()
-                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
-                optimizer.step()
-
-            if args.target_kl is not None and approx_kl > args.target_kl:
-                break
-
-        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
-        var_y = np.var(y_true)
-        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
-        writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
-        writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
-        writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
-        writer.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
-        writer.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
-        writer.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
-        writer.add_scalar("losses/explained_variance", explained_var, global_step)
-        print("SPS:", int(global_step / (time.time() - start_time)))
-        writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-
-    final_save_path = os.path.join(args.save_dir, "final_model.pt")
-    torch.save(agent.state_dict(), final_save_path)
-    print(f"Final model saved to {final_save_path}")
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/utils.py b/utils.py
index b863cef..1fcc1e2 100644
--- a/utils.py
+++ b/utils.py
@@ -5,10 +5,9 @@ def batchify_obs(obs_dict, device):
     #TODO next_obs, next_locs, next_eners = next_obs_dict["image"], next_obs_dict["location"], next_obs_dict["energy"]
     """Converts PZ style observations to batch of torch arrays."""
     # convert to list of np arrays
-
-    obs = np.array([a for a in obs_dict['image']])
-    locs = np.array([a for a in obs_dict['location']])
-    eners = np.array([a for a in obs_dict['energy']])
+    obs = np.array([obs_dict[a]['image'] for a in obs_dict])
+    locs = np.array([obs_dict[a]['location'] for a in obs_dict])
+    eners = np.array([obs_dict[a]['energy'] for a in obs_dict])
 
     # convert to torch
     obs = torch.tensor(obs).to(device)
@@ -16,7 +15,15 @@ def batchify_obs(obs_dict, device):
     eners = torch.tensor(eners).to(device)
     return obs, locs, eners
 
+def batchify(x, device):
+    #TODO next_obs, next_locs, next_eners = next_obs_dict["image"], next_obs_dict["location"], next_obs_dict["energy"]
+    """Converts PZ style observations to batch of torch arrays."""
+    # convert to list of np arrays
+    x = np.array([x[a] for a in x])
 
+    # convert to torch
+    x = torch.Tensor(x).to(device)
+    return x
 
 def unbatchify(x, num_envs, env):
     """Converts np array to PZ style arguments."""
diff --git a/visualize.py b/visualize.py
index 55e0442..47d7389 100644
--- a/visualize.py
+++ b/visualize.py
@@ -30,9 +30,9 @@ def visualize_environment(environment, step):
             pygame.draw.rect(screen, BLACK, rect, 1)
 
     # Draw home area
-    for i in range(HOME_SIZE):
-        for j in range(HOME_SIZE):
-            home_rect = pygame.Rect((HOME_POSITION[1] + j) * cell_size, (HOME_POSITION[0] + i) * cell_size, cell_size, cell_size)
+    for i in range(environment.home_size):
+        for j in range(environment.home_size):
+            home_rect = pygame.Rect((environment.home_position[1] + j) * cell_size, (environment.home_position[0] + i) * cell_size, cell_size, cell_size)
             pygame.draw.rect(screen, HOME_COLOR, home_rect)
 
     # Draw agents
@@ -59,7 +59,7 @@ def visualize_environment(environment, step):
         if len(food.carried) > 0:
             # Display "pick up" message above the food
             pickup_text = font.render("Pick Up", True, (0, 255, 0))  # Green text
-            screen.blit(pickup_text, (x, y - 20))  # Display text slightly above the food
+            screen.blit(pickup_text, (x, y+20))  # Display text slightly above the food
 
     pygame.display.flip()
 
@@ -101,9 +101,13 @@ if __name__ == "__main__":
             # print(observations)
             # if rewards[0] != 0 or rewards[1] != 0:
             #     print("reward", rewards)
-            if dones[0]:
-                print("return", env.cumulative_rewards)
-                break
+            if isinstance(dones,bool):
+                if dones:
+                    break
+            else: 
+                if dones[0]:
+                    print("return", env.cumulative_rewards)
+                    break
 
         if VISUALIZE:
             clip = ImageSequenceClip(frames, fps=5)
