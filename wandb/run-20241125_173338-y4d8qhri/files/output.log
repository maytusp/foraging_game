train_single_dqn.py:130: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:277.)
  image_seq_input = torch.tensor(image_seq, dtype=torch.float32).unsqueeze(0).to(device)
Episode 1/100000, Total Reward: 0, loss: None
Episode 2/100000, Total Reward: 0, loss: None
Episode 3/100000, Total Reward: 0, loss: None
Episode 4/100000, Total Reward: 0, loss: None
Episode 5/100000, Total Reward: 0, loss: None
Episode 6/100000, Total Reward: 0, loss: None
Episode 7/100000, Total Reward: 0, loss: None
Episode 8/100000, Total Reward: 0, loss: None
Episode 9/100000, Total Reward: 0, loss: None
Episode 10/100000, Total Reward: 0, loss: None
Episode 11/100000, Total Reward: 0, loss: None
Episode 12/100000, Total Reward: 0, loss: None
Episode 13/100000, Total Reward: 0, loss: None
Episode 14/100000, Total Reward: 0, loss: None
Episode 15/100000, Total Reward: 0, loss: None
Episode 16/100000, Total Reward: 0, loss: None
Episode 17/100000, Total Reward: 0, loss: None
Episode 18/100000, Total Reward: 0, loss: None
Episode 19/100000, Total Reward: 0, loss: None
Episode 20/100000, Total Reward: 0, loss: None
Episode 21/100000, Total Reward: 0, loss: None
Episode 22/100000, Total Reward: 0, loss: None
Episode 23/100000, Total Reward: 0, loss: None
Episode 24/100000, Total Reward: 0, loss: None
Episode 25/100000, Total Reward: 0, loss: None
Episode 26/100000, Total Reward: 0, loss: None
Episode 27/100000, Total Reward: 0, loss: None
Episode 28/100000, Total Reward: 0, loss: None
Episode 29/100000, Total Reward: 0, loss: None
Episode 30/100000, Total Reward: 0, loss: None
Episode 31/100000, Total Reward: 0, loss: None
Episode 32/100000, Total Reward: 0, loss: None
Episode 33/100000, Total Reward: 0, loss: None
Episode 34/100000, Total Reward: 0, loss: None
Episode 35/100000, Total Reward: 0, loss: None
Episode 36/100000, Total Reward: 0, loss: None
Episode 37/100000, Total Reward: 0, loss: None
Episode 38/100000, Total Reward: 0, loss: None
Episode 39/100000, Total Reward: 0, loss: None
Episode 40/100000, Total Reward: 0, loss: None
Episode 41/100000, Total Reward: 0, loss: None
Episode 42/100000, Total Reward: 0, loss: None
Episode 43/100000, Total Reward: 0, loss: None
Episode 44/100000, Total Reward: 0, loss: None
Episode 45/100000, Total Reward: 0, loss: None
Episode 46/100000, Total Reward: 0, loss: None
Episode 47/100000, Total Reward: 0, loss: None
Episode 48/100000, Total Reward: 0, loss: None
Episode 49/100000, Total Reward: 0, loss: None
Episode 50/100000, Total Reward: 0, loss: None
Episode 51/100000, Total Reward: 0, loss: None
Episode 52/100000, Total Reward: 0, loss: None
Episode 53/100000, Total Reward: 0, loss: None
Episode 54/100000, Total Reward: 0, loss: None
Episode 55/100000, Total Reward: 0, loss: None
Episode 56/100000, Total Reward: 0, loss: None
Episode 57/100000, Total Reward: 0, loss: None
Episode 58/100000, Total Reward: 0, loss: None
Episode 59/100000, Total Reward: 0, loss: None
Episode 60/100000, Total Reward: 0, loss: None
Episode 61/100000, Total Reward: 0, loss: None
Episode 62/100000, Total Reward: 0, loss: None
Episode 63/100000, Total Reward: 0, loss: None
Episode 64/100000, Total Reward: 0, loss: None
__________________________
batch.images torch.Size([64, 10, 5, 5, 4])
batch.locations torch.Size([64, 10, 2])
batch.actions torch.Size([64, 10])
batch.rewards torch.Size([64, 10])
batch.next_images torch.Size([64, 10, 5, 5, 4])
batch.next_locations torch.Size([64, 10, 2])
dones torch.Size([64, 10])
__________________________
max_next_q tensor([0.0759, 0.0769, 0.0696, 0.0735, 0.0859, 0.0845, 0.0696, 0.0750, 0.0913,
        0.0702, 0.0884, 0.0806, 0.0802, 0.0884, 0.0845, 0.0745, 0.0744, 0.0795,
        0.0804, 0.0734, 0.0806, 0.0700, 0.0915, 0.0884, 0.0808, 0.0810, 0.0706,
        0.0887, 0.0884, 0.0839, 0.0785, 0.0826, 0.0800, 0.0842, 0.0802, 0.0716,
        0.0761, 0.0909, 0.0767, 0.0753, 0.0737, 0.0778, 0.0866, 0.0799, 0.0674,
        0.0846, 0.0877, 0.0876, 0.0739, 0.0714, 0.0659, 0.0880, 0.0869, 0.0861,
        0.0879, 0.0744, 0.0741, 0.0794, 0.0807, 0.0698, 0.0798, 0.0750, 0.0755,
        0.0733])
Traceback (most recent call last):
  File "train_single_dqn.py", line 374, in <module>
    train_drqn(env, 100000)
  File "train_single_dqn.py", line 267, in train_drqn
    loss = agent.train()
  File "train_single_dqn.py", line 195, in train
    q_targets = rewards + (1 - dones) * GAMMA * max_next_q
RuntimeError: The size of tensor a (10) must match the size of tensor b (64) at non-singleton dimension 1
