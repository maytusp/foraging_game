diff --git a/.DS_Store b/.DS_Store
index 636fd93..f44b8c6 100644
Binary files a/.DS_Store and b/.DS_Store differ
diff --git a/__pycache__/models.cpython-38.pyc b/__pycache__/models.cpython-38.pyc
index 7777d7c..47ef9ed 100644
Binary files a/__pycache__/models.cpython-38.pyc and b/__pycache__/models.cpython-38.pyc differ
diff --git a/__pycache__/visualize.cpython-38.pyc b/__pycache__/visualize.cpython-38.pyc
index 6c91870..31ef672 100644
Binary files a/__pycache__/visualize.cpython-38.pyc and b/__pycache__/visualize.cpython-38.pyc differ
diff --git a/models.py b/models.py
index 08eac98..2fc6001 100644
--- a/models.py
+++ b/models.py
@@ -19,24 +19,29 @@ class PPOLSTMAgent(nn.Module):
     Agent with communication
     Observations: [image, location, energy, message]
     '''
-    def __init__(self, num_actions, grid_size=10, max_energy=200):
+    def __init__(self, num_actions, grid_size=10, max_energy=200, n_words=10, n_embedding=32):
         super().__init__()
         self.grid_size = grid_size
         self.max_energy = max_energy
-        self.image_feat_dim = 256
-        self.loc_dim = 2
-        self.energy_dim = 1
+        self.n_words = n_words
+        self.n_embedding = n_embedding
+        self.image_feat_dim = 32
+        self.loc_dim = 32
+        self.energy_dim = 32
         self.visual_encoder = nn.Sequential(nn.Flatten(), # (1,5,5) to (25)
                                         nn.Linear(25, 256), 
                                         nn.ReLU(),
                                         nn.Linear(256, 256),
                                         nn.ReLU(),
-                                        nn.Linear(256, 256),
+                                        nn.Linear(256, 128),
                                         nn.ReLU(),
-                                        nn.Linear(256, 256),
+                                        nn.Linear(128, self.image_feat_dim),
                                         nn.ReLU(),
                                         )   
-        self.lstm = nn.LSTM(self.image_feat_dim+self.loc_dim+self.energy_dim, 128)
+        self.message_encoder =  nn.Embedding(n_words, n_embedding) # Contains n_words tensor of size n_embedding
+        self.energy_encoder = nn.Linear(1, self.energy_dim)
+        self.location_encoder = nn.Linear(2, self.loc_dim)
+        self.lstm = nn.LSTM(self.image_feat_dim+self.loc_dim+self.energy_dim+self.n_embedding, 128)
         for name, param in self.lstm.named_parameters():
             if "bias" in name:
                 nn.init.constant_(param, 0)
@@ -51,9 +56,10 @@ class PPOLSTMAgent(nn.Module):
         image_feat = self.visual_encoder(image / 255.0) # (L*B, feat_dim)
         location = location / self.grid_size # (L*B,2)
         energy = energy / self.max_energy # (L*B,1)
-
+        energy_feat = self.energy_encoder(energy)
+        location_feat = self.location_encoder(location)
         # print(f"image_feat {image_feat.shape}, location {location.shape}, energy {energy.shape}, message_feat {message_feat.shape}")
-        hidden = torch.cat((image_feat, location, energy), axis=1)
+        hidden = torch.cat((image_feat, location_feat, energy_feat), axis=1)
         # print("hidden", hidden.shape)
         # LSTM logic
         hidden = hidden.reshape((-1, batch_size, self.lstm.input_size))
@@ -91,26 +97,28 @@ class PPOLSTMCommAgent(nn.Module):
     Agent with communication
     Observations: [image, location, energy, message]
     '''
-    def __init__(self, num_actions, grid_size=10, max_energy=200, n_words=10, n_embedding=3):
+    def __init__(self, num_actions, grid_size=10, max_energy=200, n_words=10, n_embedding=32):
         super().__init__()
         self.grid_size = grid_size
         self.max_energy = max_energy
         self.n_words = n_words
         self.n_embedding = n_embedding
-        self.image_feat_dim = 256
-        self.loc_dim = 2
-        self.energy_dim = 1
+        self.image_feat_dim = 32
+        self.loc_dim = 32
+        self.energy_dim = 32
         self.visual_encoder = nn.Sequential(nn.Flatten(), # (1,5,5) to (25)
                                         nn.Linear(25, 256), 
                                         nn.ReLU(),
                                         nn.Linear(256, 256),
                                         nn.ReLU(),
-                                        nn.Linear(256, 256),
+                                        nn.Linear(256, 128),
                                         nn.ReLU(),
-                                        nn.Linear(256, 256),
+                                        nn.Linear(128, self.image_feat_dim),
                                         nn.ReLU(),
                                         )   
         self.message_encoder =  nn.Embedding(n_words, n_embedding) # Contains n_words tensor of size n_embedding
+        self.energy_encoder = nn.Linear(1, self.energy_dim)
+        self.location_encoder = nn.Linear(2, self.loc_dim)
         self.lstm = nn.LSTM(self.image_feat_dim+self.loc_dim+self.energy_dim+self.n_embedding, 128)
         for name, param in self.lstm.named_parameters():
             if "bias" in name:
@@ -127,11 +135,13 @@ class PPOLSTMCommAgent(nn.Module):
         image_feat = self.visual_encoder(image / 255.0) # (L*B, feat_dim)
         location = location / self.grid_size # (L*B,2)
         energy = energy / self.max_energy # (L*B,1)
+        energy_feat = self.energy_encoder(energy)
+        location_feat = self.location_encoder(location)
         message_feat = self.message_encoder(message) # (L*B,1)
         message_feat = message_feat.view(-1, self.n_embedding)
 
         # print(f"image_feat {image_feat.shape}, location {location.shape}, energy {energy.shape}, message_feat {message_feat.shape}")
-        hidden = torch.cat((image_feat, location, energy, message_feat), axis=1)
+        hidden = torch.cat((image_feat, location_feat, energy_feat, message_feat), axis=1)
         # print("hidden", hidden.shape)
         # LSTM logic
         hidden = hidden.reshape((-1, batch_size, self.lstm.input_size))
diff --git a/test_comm_ppo.py b/test_comm_ppo.py
index 388e401..c1cce30 100644
--- a/test_comm_ppo.py
+++ b/test_comm_ppo.py
@@ -16,7 +16,7 @@ from torch.utils.tensorboard import SummaryWriter
 
 
 import supersuit as ss
-from environment_pickup import Environment
+from environment_energy_asym import Environment
 from utils import *
 from models import PPOLSTMCommAgent
 
@@ -24,7 +24,7 @@ from models import PPOLSTMCommAgent
 
 @dataclass
 class Args:
-    ckpt_path = "checkpoints/ppo_commu_ps_pickup/model_step_100M.pt"
+    ckpt_path = "checkpoints/ppo_commu_ps_energy_asym/model_step_1B.pt"
     exp_name: str = os.path.basename(__file__)[: -len(".py")]
     seed: int = 1
     torch_deterministic: bool = True
@@ -33,7 +33,7 @@ class Args:
     wandb_project_name: str = "PPO Foraging Game"
     wandb_entity: str = "maytusp"
     capture_video: bool = False
-    saved_dir = "logs/ppo_commu_ps_pickup_100M_noise_message_invisible"
+    saved_dir = "logs/ppo_comm_ps_energy_asym_1B"
     video_save_dir = os.path.join(saved_dir, "vids")
     visualize = True
     ablate_message = False
